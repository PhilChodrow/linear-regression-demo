# Introduction to Supervised Learning via Linear Least-Squares

In this demonstration lecture, I'll introduce some of the fundamental ideas in supervised learning: model families, loss functions, and iterative descent algorithms. I'll illustrate these ideas in the setting of gradient descent for univariate least-squares linear regression. This lecture is intended as a "second lecture" in a machine learning course. I assume that students have familiarity with Python programming, calculus, and several conceptual examples of "predicting Y in terms of X."  

The primary aim of the lecture is to prepare students to implement and contextualize gradient descent for least-squares linear regression in a follow-up lab activity. I offer an illustrative example of such an activity based on the jigsaw worksheet paradigm of cooperative learning: 

- https://github.com/PhilChodrow/linear-regression-demo/blob/main/lab-1.ipynb
- https://github.com/PhilChodrow/linear-regression-demo/blob/main/lab1/LinearRegression.py

After the body of the demo lecture, I'll briefly break immersion to discuss cooperative learning and its role in my pedagogical practice. 

## Speaker

Dr. Phil Chodrow is an applied mathematician, data scientist, and educator who studies network-based models of social and biological systems. His recent research includes foundations of data science in graphs and hypergraphs; inference techniques for dominance hierarchies in human and animal societies; and models of opinion dynamics on social networks. His research has been supported by the NSF Graduate Research Fellowship and the Fulbright Research Scholarship. Phil is passionate about evidence-based pedagogy, and has developed courses in computing and data science with explicit emphasis on inclusion, equity, and justice. These courses have since become standard offerings in his current program in the Department of Mathematics at the University of California, Los Angeles. Phil completed his PhD at MIT and his BA at Swarthmore College.